{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-ugJdnNHAc3"
      },
      "source": [
        "Installing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urWeHaa2lxT1",
        "outputId": "86add357-becd-4301-9b5a-b7edea66b7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.4/234.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain_openai pydantic guardrails-ai gradio duckduckgo_search -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTYYpmeiG71B"
      },
      "source": [
        "Importing Secret Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c--NIsZ_Eq74"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GROQ = userdata.get('GROQ')\n",
        "GURADRAIL = userdata.get('GURADRAIL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ekXWGPG3bj"
      },
      "source": [
        "Setting Up Guardrails Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2cMNpNyE2nl",
        "outputId": "14fab355-8070-4d3f-a4ec-3443e1c4fc0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ],
      "source": [
        "!guardrails configure --disable-remote-inferencing --disable-metrics --token {GURADRAIL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5EoNgBkGycQ"
      },
      "source": [
        "Downloading Specific Validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WtwEV0oFU4Z",
        "outputId": "efb7ff02-69b6-4949-bc06-538d0cf0c883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
            "\u001b[2K\u001b[32m[  ==]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[   =]\u001b[0m Downloading dependencies\n",
            "\u001b[2K\u001b[32m[   =]\u001b[0m Running post-install setup[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "\u001b[2K\u001b[32m[  ==]\u001b[0m Running post-install setup[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "\u001b[2K\u001b[32m[ ===]\u001b[0m Running post-install setupDownloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1.2/unbiased-albert-c8519128.ckpt\" to /root/.cache/torch/hub/checkpoints/unbiased-albert-c8519128.ckpt\n",
            "100% 44.6M/44.6M [00:00<00:00, 118MB/s]\n",
            "\u001b[2K\u001b[32m[ ===]\u001b[0m Running post-install setup2025-02-21 11:12:16.620901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2K\u001b[32m[=   ]\u001b[0m Running post-install setupWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740136336.899595    1462 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setupE0000 00:00:1740136336.975412    1462 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setup2025-02-21 11:12:17.586291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 684/684 [00:00<00:00, 4.01MB/s]\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 144kB/s]\n",
            "spiece.model: 100% 760k/760k [00:00<00:00, 3.22MB/s]\n",
            "tokenizer.json: 100% 1.31M/1.31M [00:00<00:00, 4.08MB/s]\n",
            "\u001b[2K\u001b[32m[=== ]\u001b[0m Running post-install setup\n",
            "\u001b[1A\u001b[2K✅Successfully installed guardrails/toxic_language!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import ToxicLanguage\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/toxic_language\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!guardrails hub install hub://guardrails/toxic_language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdNMooRGjkR"
      },
      "source": [
        "Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OXUaXnK_F-dn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "from duckduckgo_search import DDGS\n",
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from guardrails.hub import ToxicLanguage\n",
        "from guardrails import Guard, OnFailAction\n",
        "from langchain_core.language_models import BaseChatModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VYFIJnwGh6U"
      },
      "source": [
        "Setting up a env variable for OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0qImLO3lGKt7"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = GROQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Bk2vz3GZyH"
      },
      "source": [
        "Realtime data access tool with some business logic functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w8LBBbi1GVl0"
      },
      "outputs": [],
      "source": [
        "class DuckDuckGoSearch:\n",
        "    def __init__(self):\n",
        "        self.ddgs = DDGS()\n",
        "\n",
        "    def _fetch_results(self, query: str, max_results: int = 5) -> List[Dict]:\n",
        "        try:\n",
        "            results = self.ddgs.text(keywords=query, max_results=max_results)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching results: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_weather_forecast(self, location: str, date: str) -> List[Dict]:\n",
        "        query = f\"Weather forecast for {location} on {date}\"\n",
        "        return self._fetch_results(query, 10)\n",
        "\n",
        "    def get_hotel_bookings(\n",
        "        self, location: str, type: str, checkin: str, checkout: str, budget: str\n",
        "    ) -> List[Dict]:\n",
        "        query = f\"Hotel bookings in {location} for {type} from {checkin} to {checkout} within {budget}\"\n",
        "        return self._fetch_results(query, 10)\n",
        "\n",
        "    def get_flight_bookings(\n",
        "        self,\n",
        "        origin: str,\n",
        "        destination: str,\n",
        "        departure: str,\n",
        "        return_date: str,\n",
        "        budget: str,\n",
        "    ) -> List[Dict]:\n",
        "        query = f\"Flight bookings from {origin} to {destination} on {departure} and return on {return_date} within {budget}\"\n",
        "        return self._fetch_results(query, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMZoT2tFHKDq"
      },
      "source": [
        "Customized tools with the langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qA-R-qR_HJ1g"
      },
      "outputs": [],
      "source": [
        "# Tools\n",
        "\n",
        "\n",
        "class DestinationRequest(BaseModel):\n",
        "    \"\"\"Request destination recommendations based on user preferences.\"\"\"\n",
        "\n",
        "    destination: str = Field(..., description=\"Selected destination\")\n",
        "    num_people: int = Field(..., description=\"Number of travelers\")\n",
        "    duration_days: int = Field(..., description=\"Length of trip in days\")\n",
        "    season: str = Field(..., description=\"Preferred travel season\")\n",
        "    accommodation_level: str = Field(\n",
        "        ..., description=\"Preferred accommodation level (budget/mid-range/luxury)\"\n",
        "    )\n",
        "\n",
        "\n",
        "class ActivityPlan(BaseModel):\n",
        "    \"\"\"Request activity plan based on user preferences.\"\"\"\n",
        "\n",
        "    destination: str = Field(..., description=\"Selected destination\")\n",
        "    num_days: int = Field(..., description=\"Number of days\")\n",
        "    activities: List[str] = Field(..., description=\"Preferred activities\")\n",
        "    budget_per_day: float = Field(..., description=\"Daily budget per person\")\n",
        "\n",
        "\n",
        "class BudgetBreakdown(BaseModel):\n",
        "    \"\"\"Request budget breakdown based on user preferences.\"\"\"\n",
        "\n",
        "    destination: str = Field(..., description=\"Selected destination\")\n",
        "    num_people: int = Field(..., description=\"Number of travelers\")\n",
        "    duration_days: int = Field(..., description=\"Length of trip in days\")\n",
        "    accommodation_level: str = Field(..., description=\"Accommodation level\")\n",
        "    activities: List[str] = Field(..., description=\"Planned activities\")\n",
        "    total_budget: float = Field(..., description=\"Total budget\")\n",
        "\n",
        "\n",
        "class FlightBooking(BaseModel):\n",
        "    \"\"\"Book a flight ticket.\"\"\"\n",
        "\n",
        "    origin: str = Field(..., description=\"Origin airport\")\n",
        "    destination: str = Field(..., description=\"Destination airport\")\n",
        "    departure: str = Field(..., description=\"Departure date\")\n",
        "    return_date: str = Field(..., description=\"Return date\")\n",
        "    budget: str = Field(..., description=\"Budget for the flight\")\n",
        "\n",
        "\n",
        "class HotelBooking(BaseModel):\n",
        "    \"\"\"Book a hotel room.\"\"\"\n",
        "\n",
        "    location: str = Field(..., description=\"Location\")\n",
        "    type: str = Field(..., description=\"Type of room\")\n",
        "    checkin: str = Field(..., description=\"Check-in date\")\n",
        "    checkout: str = Field(..., description=\"Check-out date\")\n",
        "    budget: str = Field(..., description=\"Budget for the room\")\n",
        "\n",
        "\n",
        "class WeatherForecast(BaseModel):\n",
        "    \"\"\"Get weather forecast for a location.\"\"\"\n",
        "\n",
        "    location: str = Field(..., description=\"Location\")\n",
        "    date: str = Field(..., description=\"Date\")\n",
        "\n",
        "\n",
        "langchain_tools = [\n",
        "    DestinationRequest,\n",
        "    ActivityPlan,\n",
        "    BudgetBreakdown,\n",
        "    FlightBooking,\n",
        "    HotelBooking,\n",
        "    WeatherForecast,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZBPFpDTHVYv"
      },
      "source": [
        "Store, Get and Clear Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WEJKfNRxHVtm"
      },
      "outputs": [],
      "source": [
        "class ChatHistory:\n",
        "    def __init__(self, file_path: str = \"chat_history.json\"):\n",
        "        self.file_path = file_path\n",
        "        self.history = self.load_history()\n",
        "\n",
        "    def load_history(self) -> List[List[str]]:\n",
        "        \"\"\"Load chat history from file if it exists.\"\"\"\n",
        "        try:\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                return [[msg[\"user\"], msg[\"bot\"]] for msg in data]\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            return []\n",
        "\n",
        "    def save_history(self, history: List[List[str]]):\n",
        "        \"\"\"Save chat history to file.\"\"\"\n",
        "        data = [\n",
        "            {\"timestamp\": datetime.now().isoformat(), \"user\": msg[0], \"bot\": msg[1]}\n",
        "            for msg in history\n",
        "            if msg[1] is not None  # Only save complete messages\n",
        "        ]\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "            json.dump(data, f)\n",
        "\n",
        "    def clear_history(self) -> List[List[str]]:\n",
        "        \"\"\"Clear chat history and return empty list.\"\"\"\n",
        "        if os.path.exists(self.file_path):\n",
        "            os.remove(self.file_path)\n",
        "        self.history = []\n",
        "        return self.history\n",
        "\n",
        "    def get_context_string(self) -> str:\n",
        "        \"\"\"Convert chat history to a context string for the LLM.\"\"\"\n",
        "        context = []\n",
        "        for user_msg, bot_msg in self.history:\n",
        "            context.append(f\"User: {user_msg}\")\n",
        "            if bot_msg:\n",
        "                context.append(f\"Assistant: {bot_msg}\")\n",
        "        return \"\\n\".join(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4W6qi9QHfgW"
      },
      "source": [
        "Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1nFlC1yRHfXH"
      },
      "outputs": [],
      "source": [
        "VALIDATE_PROMPT = \"\"\"\n",
        "You are a travel planning assistant. Your task is to analyze the conversation and determine if you have all required information for planning.\n",
        "\n",
        "Previous Conversation:\n",
        "{conversation_context}\n",
        "\n",
        "Latest Query:\n",
        "\"{query}\"\n",
        "\n",
        "Required information:\n",
        "    - Number of travelers\n",
        "    - Budget (total or per person)\n",
        "    - Duration of trip\n",
        "    - Preferred activities or interests\n",
        "    - Time of travel (season/month)\n",
        "    - Accommodation preferences\n",
        "    - Flight details (optional)\n",
        "\n",
        "Analyze the conversation history and latest query. Check if all required information is available.\n",
        "\n",
        "You must respond with a valid JSON object in the following format, and ONLY this format:\n",
        "{{\n",
        "\"is_complete\": false,\n",
        "\"missing_info\": [\"list\", \"of\", \"missing\", \"details\"],\n",
        "\"follow_up_question\": \"your follow-up question here\"\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "1. The response must be a valid JSON object\n",
        "2. All fields must be present\n",
        "3. \"is_complete\" must be boolean\n",
        "4. \"missing_info\" must be an array of strings\n",
        "5. \"follow_up_question\" must be a string\n",
        "\n",
        "Consider previous messages when checking for information. If a detail was provided earlier, don't ask for it again.\n",
        "Make the follow_up_question natural and conversational.\n",
        "\"\"\"\n",
        "\n",
        "TOOL_PROMPT = \"\"\"\n",
        "Create a detailed travel plan based on the following conversation:\n",
        "\n",
        "Previous Conversation:\n",
        "{conversation_context}\n",
        "\n",
        "Latest Query:\n",
        "{query}\n",
        "\n",
        "Tool Name and Usage:\n",
        "    - Destination Request: Recommend destinations based on user preferences\n",
        "    - Activity Plan: Create an activity plan based on user preferences\n",
        "    - Budget Breakdown: Provide a budget breakdown based on user preferences\n",
        "    - Flight Booking: If the user wants to book a flight ticket (optional)\n",
        "    - Hotel Booking: If the user wants to book a hotel room (optional)\n",
        "    - Weather Forecast: If the user wants forecast for a location (optional)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "FORMAT_PROMPT = \"\"\"\n",
        "Based on the following conversation history and tool responses.\n",
        "\n",
        "Previous Conversation:\n",
        "{conversation_context}\n",
        "\n",
        "Latest Query:\n",
        "{query}\n",
        "\n",
        "Tool Responses:\n",
        "{tool_responses}\n",
        "\n",
        "Additional API Responses:\n",
        "{additional_api_responses}\n",
        "\n",
        "IMPORTANT NOTE:\n",
        "- Do not mention API names or technical details in the response.\n",
        "\n",
        "If conversation context is available then consider the entire conversation context when creating the plan. Reference and build upon any preferences or details mentioned in earlier messages.\n",
        "Make the content informative yet engaging while maintaining a professional tone.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfpQeeyVHl_o"
      },
      "source": [
        "Chat core logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y5tUZol_Hl3L"
      },
      "outputs": [],
      "source": [
        "class Chat:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tools: List[BaseModel],\n",
        "        chat_model: BaseChatModel,\n",
        "        ddgs: DuckDuckGoSearch = DuckDuckGoSearch(),\n",
        "    ):\n",
        "        self.ddgs = ddgs\n",
        "        self.chat_history = ChatHistory()\n",
        "        self.chat_model = chat_model\n",
        "        self.chat_model_tools = self.chat_model.bind_tools(tools)\n",
        "        self.guard = Guard().use(\n",
        "            ToxicLanguage(\n",
        "                threshold=0.5,\n",
        "                validation_method=\"sentence\",\n",
        "                on_fail=OnFailAction.EXCEPTION,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _ddgs_search(self, tool_response: str) -> str:\n",
        "        \"\"\"Execute search queries based on tool response.\"\"\"\n",
        "        try:\n",
        "            response = \"\"\n",
        "            parsed_response = json.loads(tool_response)\n",
        "\n",
        "            tool_calls = []\n",
        "            if isinstance(parsed_response, dict):\n",
        "                if \"tool_calls\" in parsed_response:\n",
        "                    tool_calls = parsed_response[\"tool_calls\"]\n",
        "                elif \"additional_kwargs\" in parsed_response:\n",
        "                    if \"tool_calls\" in parsed_response[\"additional_kwargs\"]:\n",
        "                        tool_calls = parsed_response[\"additional_kwargs\"][\"tool_calls\"]\n",
        "\n",
        "            if not tool_calls:\n",
        "                return \"\"\n",
        "\n",
        "            for tool_call in tool_calls:\n",
        "                # Extract function details safely\n",
        "                function_details = tool_call.get(\"function\", {})\n",
        "                name = function_details.get(\"name\", \"\")\n",
        "                arguments = function_details.get(\"arguments\", \"{}\")\n",
        "\n",
        "                try:\n",
        "                    args = (\n",
        "                        json.loads(arguments)\n",
        "                        if isinstance(arguments, str)\n",
        "                        else arguments\n",
        "                    )\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "                if name == \"HotelBooking\":\n",
        "                    details = self.ddgs.get_hotel_bookings(\n",
        "                        args.get(\"location\", \"\"),\n",
        "                        args.get(\"type\", \"\"),\n",
        "                        args.get(\"checkin\", \"\"),\n",
        "                        args.get(\"checkout\", \"\"),\n",
        "                        args.get(\"budget\", \"\"),\n",
        "                    )\n",
        "                    if details:\n",
        "                        response += f\"Hotel Details: {json.dumps(details)}\\n\"\n",
        "\n",
        "                elif name == \"WeatherForecast\":\n",
        "                    details = self.ddgs.get_weather_forecast(\n",
        "                        args.get(\"location\", \"\"), args.get(\"date\", \"\")\n",
        "                    )\n",
        "                    if details:\n",
        "                        response += f\"Weather Forecast: {json.dumps(details)}\\n\"\n",
        "\n",
        "                elif name == \"FlightBooking\":\n",
        "                    details = self.ddgs.get_flight_bookings(\n",
        "                        args.get(\"origin\", \"\"),\n",
        "                        args.get(\"destination\", \"\"),\n",
        "                        args.get(\"departure\", \"\"),\n",
        "                        args.get(\"return_date\", \"\"),\n",
        "                        args.get(\"budget\", \"\"),\n",
        "                    )\n",
        "                    if details:\n",
        "                        response += f\"Flight Details: {json.dumps(details)}\\n\"\n",
        "\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching DDGS search results: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _format_context(self, history: List[List[str]]) -> str:\n",
        "        \"\"\"Format conversation history into a context string.\"\"\"\n",
        "        context = []\n",
        "        for user_msg, bot_msg in history:\n",
        "            context.append(f\"User: {user_msg}\")\n",
        "            if bot_msg:\n",
        "                context.append(f\"Assistant: {bot_msg}\")\n",
        "        return \"\\n\".join(context)\n",
        "\n",
        "    def _validate_query_information(self, query: str, history: List[List[str]]) -> Dict:\n",
        "        \"\"\"Validate if all required travel information is provided.\"\"\"\n",
        "        history_context = self._format_context(history)\n",
        "\n",
        "        try:\n",
        "            validate_prompt = VALIDATE_PROMPT.format(\n",
        "                conversation_context=history_context, query=query\n",
        "            )\n",
        "            response = self.chat_model.invoke(validate_prompt)\n",
        "            parsed_response = json.loads(response.content)\n",
        "\n",
        "            required_keys = {\"is_complete\", \"missing_info\", \"follow_up_question\"}\n",
        "            if not all(key in parsed_response for key in required_keys):\n",
        "                return {\n",
        "                    \"is_complete\": False,\n",
        "                    \"missing_info\": [\"invalid_response\"],\n",
        "                    \"follow_up_question\": \"Could you please provide more details about your travel plans?\",\n",
        "                }\n",
        "\n",
        "            return parsed_response\n",
        "        except json.JSONDecodeError:\n",
        "            return {\n",
        "                \"is_complete\": False,\n",
        "                \"missing_info\": [\"error\"],\n",
        "                \"follow_up_question\": \"Could you please provide more details about your travel plans?\",\n",
        "            }\n",
        "\n",
        "    def _execute_tool_response(self, query: str, history: List[List[str]]) -> str:\n",
        "        \"\"\"Execute tools based on the validated query.\"\"\"\n",
        "        try:\n",
        "            history_context = self._format_context(history)\n",
        "            tool_prompt = TOOL_PROMPT.format(\n",
        "                conversation_context=history_context, query=query\n",
        "            )\n",
        "\n",
        "            response = self.chat_model_tools.invoke(tool_prompt)\n",
        "            return json.dumps(response.additional_kwargs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing tool response: {str(e)}\")\n",
        "            return \"I apologize, but I'm having trouble processing your request. Could you please try again?\"\n",
        "\n",
        "    def generate_response(self, query: str, history: List[List[str]]) -> str:\n",
        "        \"\"\"Generate a complete travel plan response.\"\"\"\n",
        "        try:\n",
        "            # Guardrails validation for toxic language\n",
        "            try:\n",
        "                self.guard.validate(query)\n",
        "            except Exception:\n",
        "                return (\n",
        "                    f\"Please refrain from using inappropriate language in your messages like '{query}'. Thank you!\",\n",
        "                    \"\",\n",
        "                    \"\",\n",
        "                )\n",
        "\n",
        "            tool_response = self._execute_tool_response(query, history)\n",
        "            history_context = self._format_context(history)\n",
        "            additional_api_responses = self._ddgs_search(tool_response)\n",
        "\n",
        "            formatted_prompt = FORMAT_PROMPT.format(\n",
        "                conversation_context=history_context,\n",
        "                query=query,\n",
        "                tool_responses=tool_response,\n",
        "                additional_api_responses=additional_api_responses,\n",
        "            )\n",
        "\n",
        "            response = self.chat_model.invoke(formatted_prompt)\n",
        "            return response.content, tool_response, additional_api_responses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {str(e)}\")\n",
        "            return (\n",
        "                \"I apologize, but I encountered an error while creating your travel plan. Please try again with your requirements.\",\n",
        "                tool_response if tool_response else \"\",\n",
        "                additional_api_responses if additional_api_responses else \"\",\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79WShfO2H6ul"
      },
      "source": [
        "Interface Setup and addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4xM0sb8cH7GH"
      },
      "outputs": [],
      "source": [
        "def create_interface():\n",
        "    llm = ChatOpenAI(\n",
        "        base_url=\"https://api.groq.com/openai/v1\", model=\"llama-3.2-90b-vision-preview\"\n",
        "    )\n",
        "    chat_process = Chat(tools=langchain_tools, chat_model=llm)\n",
        "\n",
        "    def user_message(\n",
        "        message: str,\n",
        "        history: List[List[str]],\n",
        "        tool_responses: List[str],\n",
        "        api_responses: List[str],\n",
        "    ) -> Tuple[str, List[List[str]], str, str]:\n",
        "        history = history or []\n",
        "        history.append([message, None])\n",
        "        return \"\", history, \"\", \"\"\n",
        "\n",
        "    def format_json(json_str: str) -> str:\n",
        "        \"\"\"Format JSON string for better readability.\"\"\"\n",
        "        try:\n",
        "            parsed = json.loads(json_str)\n",
        "            return json.dumps(parsed, indent=2)\n",
        "        except Exception:\n",
        "            return json_str\n",
        "\n",
        "    def bot_message(\n",
        "        history: List[List[str]], tool_responses: List[str], api_responses: List[str]\n",
        "    ) -> Tuple[List[List[str]], str, str]:\n",
        "        if history and history[-1][1] is None:\n",
        "            previous_history = [h for h in history[:-1] if h[1] is not None]\n",
        "            response, tool_resp, api_resp = chat_process.generate_response(\n",
        "                history[-1][0], previous_history\n",
        "            )\n",
        "            history[-1][1] = response\n",
        "            chat_process.chat_history.save_history(history)\n",
        "\n",
        "            # Format the responses\n",
        "            formatted_tool_resp = format_json(tool_resp)\n",
        "            formatted_api_resp = format_json(api_resp)\n",
        "\n",
        "            return history, formatted_tool_resp, formatted_api_resp\n",
        "        return history, \"\", \"\"\n",
        "\n",
        "    example_messages = [\n",
        "        \"I'm planning a trip to Europe with 4 friends. We have a budget of $2000 per person and want to visit historical sites and museums. We're flexible with the travel dates.\",\n",
        "        \"I'm looking for a beach vacation in the Caribbean for 2 people. Our budget is $3000 for a week-long trip. We enjoy water sports and relaxing on the beach.\",\n",
        "        \"Why some people are so dumb to listen\",\n",
        "        \"I need a cheaper flight from chennai to madurai\",\n",
        "        \"I want to go on a hiking trip in the mountains for 5 days. My budget is $1000 and I prefer to stay in budget accommodations.\",\n",
        "        \"I want to go on a solo hiking trip to Mt. Everest for 10 days on June 1st. I have a budget of $5000 and I'm looking to climb the base camp. I would like to stay in mid-range accommodations like lodges. I don't want any activities beside hiking. I'm doing hiking for about 5 years and very fit. I'll bring my own medical equiments. Can you help me plan this trip?\",\n",
        "    ]\n",
        "\n",
        "    with gr.Blocks(title=\"Octonomy - Workshop 1\", theme=gr.themes.Default(primary_hue=gr.themes.colors.violet, secondary_hue=gr.themes.colors.purple)) as demo:\n",
        "        # Header with logo and name\n",
        "        with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.HTML(\n",
        "                      \"\"\"\n",
        "                      <a href=\"https://www.octonomy.ai/\" target=\"_blank\">\n",
        "                          <img src=\"https://www.octonomy.ai/wp-content/uploads/2025/01/octonomy-Light-Logo-1.svg\"\n",
        "                              style=\"width: 150px; cursor: pointer;\">\n",
        "                      </a>\n",
        "                      \"\"\"\n",
        "                  )\n",
        "        # Control for showing/hiding responses\n",
        "        with gr.Row():\n",
        "            show_responses = gr.Checkbox(\n",
        "                label=\"Show Response Details\", value=False, interactive=True\n",
        "            )\n",
        "\n",
        "        # Main container\n",
        "        with gr.Row():\n",
        "            # Chat container (dynamic width based on checkbox)\n",
        "            chat_container = gr.Column(scale=1, visible=True)\n",
        "            with chat_container:\n",
        "                chatbot = gr.Chatbot(\n",
        "                    value=chat_process.chat_history.load_history(),\n",
        "                    height=480,\n",
        "                    show_label=False,\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg = gr.Textbox(\n",
        "                        placeholder=\"Enter your travel requirements...\",\n",
        "                        show_label=False,\n",
        "                        scale=6,\n",
        "                        container=False,\n",
        "                    )\n",
        "                    submit = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
        "                    clear = gr.Button(\"Clear Chat\", scale=1)  \n",
        "\n",
        "                with gr.Row():\n",
        "                    \n",
        "                    gr.Examples(\n",
        "                        examples=example_messages,\n",
        "                        inputs=msg,\n",
        "                        label=\"Example Queries\",\n",
        "                        examples_per_page=2,\n",
        "                    )\n",
        "\n",
        "            # Response panels (visibility toggled by checkbox)\n",
        "            responses_container = gr.Column(scale=1, visible=False)\n",
        "            with responses_container:\n",
        "                with gr.Tabs():\n",
        "                    with gr.Tab(\"Tool Details\"):\n",
        "                        tool_responses = gr.Code(\n",
        "                            label=\"Processing Details\",\n",
        "                            language=\"json\",\n",
        "                            show_label=False,\n",
        "                            lines=15,\n",
        "                        )\n",
        "                    with gr.Tab(\"API Results\"):\n",
        "                        api_responses = gr.Code(\n",
        "                            label=\"External Data\",\n",
        "                            language=\"json\",\n",
        "                            show_label=False,\n",
        "                            lines=15,\n",
        "                        )\n",
        "\n",
        "        # Toggle response panels visibility\n",
        "        def toggle_responses(show: bool):\n",
        "            return {\n",
        "                responses_container: gr.update(visible=show),\n",
        "                chat_container: gr.update(scale=2 if show else 1),\n",
        "            }\n",
        "\n",
        "        show_responses.change(\n",
        "            toggle_responses,\n",
        "            show_responses,\n",
        "            [responses_container, chat_container],\n",
        "        )\n",
        "\n",
        "        # Event handlers\n",
        "        msg.submit(\n",
        "            user_message,\n",
        "            [msg, chatbot, tool_responses, api_responses],\n",
        "            [msg, chatbot, tool_responses, api_responses],\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            bot_message,\n",
        "            [chatbot, tool_responses, api_responses],\n",
        "            [chatbot, tool_responses, api_responses],\n",
        "        )\n",
        "\n",
        "        submit.click(\n",
        "            user_message,\n",
        "            [msg, chatbot, tool_responses, api_responses],\n",
        "            [msg, chatbot, tool_responses, api_responses],\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            bot_message,\n",
        "            [chatbot, tool_responses, api_responses],\n",
        "            [chatbot, tool_responses, api_responses],\n",
        "        )\n",
        "\n",
        "        def clear_all():\n",
        "            chat_process.chat_history.clear_history()\n",
        "            return [], \"\", \"\"\n",
        "\n",
        "        clear.click(\n",
        "            clear_all,\n",
        "            None,\n",
        "            [chatbot, tool_responses, api_responses],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2FVyYgJIHjl"
      },
      "source": [
        "Launch the Chat window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ul8TLq__IYyi"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvmVFD43IJbF",
        "outputId": "560255be-7d28-48fd-ed61-f7461441b58c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6122c2f037a64ffe9f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.queue()\n",
        "    demo.launch(inline=False, share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
